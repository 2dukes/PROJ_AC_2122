{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_full = pd.read_csv(\"./preprocessed/mergedTestData.csv\")\n",
    "\n",
    "def run_model(pipeline, param_grid, X_train, y_train, X_test):\n",
    "  # Define evaluation procedure\n",
    "  cv = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "  grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, \n",
    "    scoring='roc_auc', n_jobs=-1, cv=cv)\n",
    "\n",
    "  grid_result = grid.fit(X_train, y_train)\n",
    "  print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "  # Evaluate the model\n",
    "  p_pred = grid_result.predict_proba(X_test)\n",
    "\n",
    "  resultData = {'Id': X_test_full['loan_id'], 'Predicted': p_pred[:,1]}\n",
    "  result = pd.DataFrame(data=resultData)\n",
    "  return result\n",
    "\n",
    "def encode_df(df):\n",
    "  columnsToEncode = list(df.select_dtypes(include=['object']))\n",
    "  le = LabelEncoder()\n",
    "  for feature in columnsToEncode:\n",
    "      try:\n",
    "          df[feature] = le.fit_transform(df[feature])\n",
    "      except:\n",
    "          print('Error encoding ' + feature)\n",
    "  return df\n",
    "\n",
    "def load_data():\n",
    "  X_train_full = pd.read_csv(\"./preprocessed/mergedTrainData.csv\")\n",
    "\n",
    "  # Obtain target and predictors\n",
    "  # features = [\"duration\",\"payments\",\"last_balance\",\"itr_balance_per_account\", \"frequency\", \"region\",\"ratio entrepeneurs\",\"average salary \",\"unemploymant_growth\",\"criminality_growth\",\"age_group\"]\n",
    "  # features = ['reached_negative_balance', 'ratio_RAB', 'credit_ratio', 'withdrawal_ratio', 'IC_mean', 'balance_min', 'mean_trans_profit', 'balance_mean', 'ratio_CC', 'CC_std', 'ratio_IC', 'withdrawal_max', 'WC_max']\n",
    "  features = ['reached_negative_balance', 'RAB_mean', 'ratio_RAB', 'credit_ratio', 'balance_min', 'RAB_sum', 'mean_trans_profit', 'balance_mean', 'ratio_CC', 'CC_std', 'IC_min', 'CC_max', 'ratio_IC', 'withdrawal_max', 'WC_max', 'last_balance', 'CAB_mean']\n",
    "\n",
    "  X_train = X_train_full[features]\n",
    "  X_test = X_test_full[features]\n",
    "  y_train = X_train_full.status\n",
    "\n",
    "  return [X_train, X_test, y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- WITH UNDERSAMPLING -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:437: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:437: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:478: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:437: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53405/688854348.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m           'classification__class_weight': [\"balanced\", None]}\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./results/logisticRegressionUndersampling.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_53405/1266911149.py\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(pipeline, param_grid, X_train, y_train, X_test)\u001b[0m\n\u001b[1;32m      8\u001b[0m     scoring='roc_auc', n_jobs=-1, cv=cv)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best: %f using %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "[X_train, X_test, y_train] = load_data()\n",
    "\n",
    "X_train = encode_df(X_train)\n",
    "X_test = encode_df(X_test)\n",
    "\n",
    "# With Undersampling\n",
    "print(\"------- WITH UNDERSAMPLING -------\")\n",
    "pipeline = Pipeline([('under', RandomUnderSampler()), ('classification', LogisticRegression(random_state=0, max_iter=10000))])\n",
    "\n",
    "param_grid = {'under__sampling_strategy': [0.1, 0.25, 0.5, 0.75, 0.9, 0.99, 'majority'],\n",
    "          'classification__penalty': ['l1', 'l2', 'none'],\n",
    "          'classification__C': [0.01, 0.05, 0.1, 0.2, 0.5, 1.0],\n",
    "          'classification__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "          'classification__class_weight': [\"balanced\", None]}\n",
    "\n",
    "results_df = run_model(pipeline, param_grid, X_train, y_train, X_test)\n",
    "results_df.to_csv(\"./results/logisticRegressionUndersampling.csv\", index=None)\n",
    "\n",
    "print(\"-----------------------\\n\\n\")\n",
    "\n",
    "# Without Oversampling & Undersampling\n",
    "print(\"------- WITHOUT OVERSAMPLING & UNDERSAMPLING -------\")\n",
    "pipeline = Pipeline([('classification', LogisticRegression(random_state=0, max_iter=10000))])\n",
    "\n",
    "param_grid = {'classification__penalty': ['l1', 'l2', 'none'],\n",
    "          'classification__C': [0.01, 0.05, 0.1, 0.2, 0.5, 1.0],\n",
    "          'classification__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "          'classification__class_weight': [\"balanced\", None]}\n",
    "\n",
    "results_df = run_model(pipeline, param_grid, X_train, y_train, X_test)\n",
    "results_df.to_csv(\"./results/logisticRegression.csv\", index=None)\n",
    "\n",
    "print(\"-----------------------\\n\\n\")\n",
    "\n",
    "# With Oversampling\n",
    "print(\"------- WITH OVERSAMPLING -------\")\n",
    "pipeline = Pipeline([('smote', SMOTE()), ('classification', LogisticRegression(random_state=0, max_iter=10000))])\n",
    "\n",
    "weights = list(np.linspace(0.005, 0.25, 5)) + ['minority', 'auto']\n",
    "param_grid = {\n",
    "        'smote__sampling_strategy': weights,\n",
    "        'classification__penalty': ['l1', 'l2', 'none'],\n",
    "        'classification__C': [0.001, 0.01, 0.05, 0.1],\n",
    "        'classification__solver': ['liblinear', 'newton-cg', 'saga'],\n",
    "        'classification__class_weight': [\"balanced\", None]}\n",
    "\n",
    "results_df = run_model(pipeline, param_grid, X_train, y_train, X_test)\n",
    "results_df.to_csv(\"./results/logisticRegressionWithSMOTE.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/30814231/using-the-predict-proba-function-of-randomforestclassifier-in-the-safe-and-rig\n",
    "# https://rpmcruz.github.io/machine%20learning/2018/02/09/probabilities-trees.html\n",
    "\n",
    "# Load data\n",
    "[X_train, X_test, y_train] = load_data()\n",
    "\n",
    "# With Undersampling\n",
    "print(\"------- WITH UNDERSAMPLING -------\")\n",
    "pipeline = Pipeline([('under', RandomUnderSampler()), ('classification', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "param_grid = {'under__sampling_strategy': [0.1, 0.25, 0.5, 0.75, 0.9, 0.99, 'majority'],\n",
    "            'classification__n_estimators': [int(x) for x in range(2, 14, 2)],\n",
    "            'classification__max_features': ['auto', 'sqrt'],\n",
    "            'classification__max_depth': [2, 6, 10, 14],\n",
    "            'classification__criterion': ['gini', 'entropy'],\n",
    "            'classification__min_samples_split':  [2, 4, 6, 8],\n",
    "            'classification__min_samples_leaf':  [1, 2, 4, 6],\n",
    "            'classification__class_weight': [\"balanced\", \"balanced_subsample\", None]}\n",
    "\n",
    "results_df = run_model(pipeline, param_grid, X_train, y_train, X_test)\n",
    "results_df.to_csv(\"./results/randomForestUndersampling.csv\", index=None)\n",
    "\n",
    "print(\"-----------------------\\n\\n\")\n",
    "\n",
    "# Without Oversampling & Undersampling\n",
    "print(\"------- WITHOUT OVERSAMPLING & UNDERSAMPLING -------\")\n",
    "param_grid = {'classification__n_estimators': [int(x) for x in range(2, 14, 2)],\n",
    "            'classification__max_features': ['auto', 'sqrt'],\n",
    "            'classification__max_depth': [2, 6, 10, 14],\n",
    "            'classification__criterion': ['gini', 'entropy'],\n",
    "            'classification__min_samples_split':  [2, 4, 6, 8],\n",
    "            'classification__min_samples_leaf':  [1, 2, 4, 6],\n",
    "            'classification__class_weight': [\"balanced\", \"balanced_subsample\", None]}\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('classification', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "results_df = run_model(pipeline, param_grid, X_train, y_train, X_test)\n",
    "results_df.to_csv(\"./results/randomForest.csv\", index=None)\n",
    "\n",
    "print(\"-----------------------\\n\\n\")\n",
    "\n",
    "# With Oversampling\n",
    "print(\"------- WITH OVERSAMPLING -------\")\n",
    "pipeline = Pipeline([('smote', SMOTE()), ('classification', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "weights = list(np.linspace(0.005, 0.25, 10)) + ['minority', 'auto']\n",
    "param_grid['smote__sampling_strategy'] = weights\n",
    "results_df = run_model(pipeline, param_grid)\n",
    "results_df.to_csv(\"./results/randomForestWithSMOTE.csv\", index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "Applying Min-Max Scaling so that values are normalized (0-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(df, numeric_columns):\n",
    "  # copy the dataframe\n",
    "  df_norm = df.copy()\n",
    "  # apply min-max scaling\n",
    "  for column in numeric_columns:\n",
    "      df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
    "      \n",
    "  return df_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- WITH UNDERSAMPLING -------\n",
      "Best: 0.889980 using {'classification__C': 100, 'classification__gamma': 0.001, 'classification__kernel': 'sigmoid', 'under__sampling_strategy': 0.75}\n",
      "     reached_negative_balance  RAB_mean  ratio_RAB  credit_ratio  balance_min  \\\n",
      "0                         1.0  0.699383   0.546415      0.146538     0.474157   \n",
      "1                         0.0  0.793889   0.754731      0.191233     0.433774   \n",
      "2                         1.0  0.918482   0.887108      0.027116     0.458202   \n",
      "3                         1.0  0.940499   0.865191      0.093363     0.479476   \n",
      "4                         1.0  0.790687   0.359231      0.299821     0.476817   \n",
      "..                        ...       ...        ...           ...          ...   \n",
      "349                       1.0  1.000000   0.000000      0.379277     0.474157   \n",
      "350                       1.0  1.000000   0.000000      0.274898     0.479476   \n",
      "351                       1.0  1.000000   0.000000      0.195359     0.482135   \n",
      "352                       1.0  1.000000   0.000000      0.277235     0.466179   \n",
      "353                       1.0  0.400685   0.277282      0.291916     0.471498   \n",
      "\n",
      "      RAB_sum  mean_trans_profit  balance_mean  ratio_CC    CC_std    IC_min  \\\n",
      "0    0.613525           0.660156      0.663113  0.175218  0.483870  0.373467   \n",
      "1    0.679238           0.572825      0.352906  0.174929  0.726426  0.070423   \n",
      "2    0.663539           0.764744      0.469661  0.011585  0.224621  0.207633   \n",
      "3    0.903375           0.674889      0.527183  0.022770  0.006688  0.246252   \n",
      "4    0.773395           0.471897      0.200691  0.285526  0.096925  0.089959   \n",
      "..        ...                ...           ...       ...       ...       ...   \n",
      "349  1.000000           0.466007      0.343187  0.245833  0.775469  0.172649   \n",
      "350  1.000000           0.574118      0.732757  0.283397  0.710313  0.049523   \n",
      "351  1.000000           0.683784      0.558395  0.023451  0.349973  0.396638   \n",
      "352  1.000000           0.516389      0.327649  0.065010  0.257304  0.167197   \n",
      "353  0.472829           0.491927      0.282104  0.162037  0.307423  0.093139   \n",
      "\n",
      "       CC_max  ratio_IC  withdrawal_max    WC_max  last_balance  CAB_mean  \n",
      "0    0.731809  0.334270        0.999327  0.999327      0.456208  0.000000  \n",
      "1    0.836918  0.408654        0.999788  0.999788      0.117237  0.000000  \n",
      "2    0.190798  0.241477        0.995392  0.995392      0.422345  0.574792  \n",
      "3    0.020084  0.269366        0.999908  0.999327      0.361298  0.438695  \n",
      "4    0.172722  0.354167        0.999327  0.999327      0.182821  0.000000  \n",
      "..        ...       ...             ...       ...           ...       ...  \n",
      "349  0.982125  0.569196        0.999327  0.999327      0.334185  0.000000  \n",
      "350  0.999618  0.317529        0.999327  0.999327      0.380235  0.000000  \n",
      "351  0.283184  0.367788        0.998618  0.998618      0.441999  0.679028  \n",
      "352  0.309293  0.391447        0.999327  0.999327      0.357155  0.267799  \n",
      "353  0.505995  0.602083        0.999327  0.999327      0.190848  0.000000  \n",
      "\n",
      "[354 rows x 17 columns]\n",
      "-----------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "1000 fits failed out of a total of 7000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1000 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dukes/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/dukes/.local/lib/python3.9/site-packages/imblearn/pipeline.py\", line 262, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/dukes/.local/lib/python3.9/site-packages/imblearn/pipeline.py\", line 220, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/home/dukes/.local/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/dukes/.local/lib/python3.9/site-packages/imblearn/pipeline.py\", line 388, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/home/dukes/.local/lib/python3.9/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/home/dukes/.local/lib/python3.9/site-packages/imblearn/utils/_validation.py\", line 535, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/home/dukes/.local/lib/python3.9/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/dukes/.local/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.84232804 0.85837302 0.86290344 0.85537698 0.84827381\n",
      " 0.85929894        nan 0.87539683 0.84722884 0.81353175 0.84614418\n",
      " 0.82403439 0.83988095        nan 0.88724206 0.88642857 0.86204365\n",
      " 0.86140873 0.8664881  0.85050926        nan 0.72616402 0.64066799\n",
      " 0.66377646 0.7390873  0.74686508 0.73290344        nan 0.85916667\n",
      " 0.86571429 0.8604828  0.85333995 0.84537698 0.84973545        nan\n",
      " 0.82881614 0.84373677 0.86554894 0.82770503 0.83652778 0.79990079\n",
      "        nan 0.87047619 0.87743386 0.87590608 0.82816138 0.83944444\n",
      " 0.8536045         nan 0.84208333 0.84396164 0.85666005 0.85611772\n",
      " 0.84411376 0.83479497        nan 0.82720899 0.85294974 0.85873016\n",
      " 0.83974868 0.84768519 0.82428571        nan 0.82338624 0.84631614\n",
      " 0.8532209  0.81353836 0.80962963 0.82585979        nan 0.85267857\n",
      " 0.86417989 0.86798942 0.86031746 0.85210979 0.84114418        nan\n",
      " 0.85351852 0.8725     0.84304894 0.8525463  0.83830688 0.85654101\n",
      "        nan 0.86285053 0.85013889 0.86814153 0.8445172  0.85294312\n",
      " 0.86376323        nan 0.81837963 0.84654762 0.84988757 0.82950397\n",
      " 0.85339286 0.84180556        nan 0.83955026 0.86822751 0.86554894\n",
      " 0.83579365 0.83093915 0.83718915        nan 0.83324735 0.86468915\n",
      " 0.85113095 0.85671296 0.83919312 0.85135582        nan 0.83332011\n",
      " 0.85468254 0.87314815 0.86082011 0.84672619 0.85042328        nan\n",
      " 0.81193783 0.8444246  0.85407407 0.83064153 0.82787698 0.85043651\n",
      "        nan 0.82335979 0.87051587 0.8301455  0.85087963 0.8357209\n",
      " 0.83429233        nan 0.82053571 0.86847884 0.83115741 0.85109127\n",
      " 0.82087302 0.84419312        nan 0.80359788 0.84926587 0.86987434\n",
      " 0.87012566 0.85739418 0.84278439        nan 0.8355291  0.81907407\n",
      " 0.81053571 0.8171164  0.79814153 0.83042328        nan 0.86150794\n",
      " 0.85241402 0.86783069 0.86214286 0.87282407 0.84097884        nan\n",
      " 0.52902778 0.55401455 0.61752646 0.68806217 0.75577381 0.77410714\n",
      "        nan 0.81911376 0.85125    0.86499339 0.84374339 0.85589286\n",
      " 0.85111772        nan 0.81785714 0.84700397 0.84292989 0.83699074\n",
      " 0.83447751 0.85732804        nan 0.86109788 0.87893519 0.88066799\n",
      " 0.86592593 0.87835317 0.87656746        nan 0.83270503 0.87445767\n",
      " 0.85963624 0.8546627  0.86105159 0.85150132        nan 0.83830026\n",
      " 0.85059524 0.8572619  0.86259921 0.84505291 0.8299537         nan\n",
      " 0.80134921 0.84996693 0.83869709 0.8317791  0.83028439 0.83624339\n",
      "        nan 0.85084656 0.8862963  0.86659392 0.8783664  0.85963624\n",
      " 0.8390873         nan 0.84732804 0.86943783 0.87439153 0.84203704\n",
      " 0.84521164 0.84233466        nan 0.81476852 0.84988757 0.85106481\n",
      " 0.85493386 0.86214286 0.87415344        nan 0.83595899 0.83811508\n",
      " 0.83541667 0.81861111 0.83059524 0.82196429        nan 0.80983466\n",
      " 0.86085317 0.86361111 0.8337037  0.85170635 0.86345899        nan\n",
      " 0.84998677 0.85327381 0.84205026 0.83938492 0.86511243 0.84722884\n",
      "        nan 0.8160119  0.85178571 0.86089947 0.87257275 0.84956349\n",
      " 0.85278439        nan 0.78452381 0.85749339 0.83705026 0.81669312\n",
      " 0.81273148 0.81324735        nan 0.83112434 0.87611111 0.85563492\n",
      " 0.85251323 0.84945106 0.83505291        nan 0.83774471 0.85000661\n",
      " 0.86207011 0.8505754  0.83079365 0.84794974        nan 0.83668651\n",
      " 0.85830688 0.87179233 0.86780423 0.84843254 0.87054894        nan\n",
      " 0.82339286 0.82419974 0.77983466 0.79900794 0.76508598 0.7538955\n",
      "        nan 0.85054894 0.82589947 0.83025794 0.79912698 0.84759259\n",
      " 0.80437831        nan 0.5299537  0.55768519 0.62083995 0.70421958\n",
      " 0.73797619 0.75179894        nan 0.84537698 0.85434524 0.86054233\n",
      " 0.8745172  0.83811508 0.86105159        nan 0.82565476 0.83527116\n",
      " 0.83677249 0.84687169 0.85492725 0.83068122        nan 0.84643519\n",
      " 0.8510119  0.86244709 0.86703042 0.85982143 0.87072751        nan\n",
      " 0.81621693 0.85017196 0.86029101 0.86308862 0.85261905 0.8721627\n",
      "        nan 0.83744709 0.85837302 0.8475463  0.84402778 0.82554233\n",
      " 0.83337963        nan 0.82962302 0.85420635 0.84136243 0.82780423\n",
      " 0.82806878 0.82611772        nan 0.83700397 0.86345238 0.86051587\n",
      " 0.87786376 0.84380952 0.85079365        nan 0.8460119  0.86266534\n",
      " 0.8771164  0.86373016 0.85568122 0.8478836         nan 0.83862434\n",
      " 0.8502381  0.84776455 0.86673942 0.87977513 0.83835979        nan\n",
      " 0.81642196 0.86919974 0.84019841 0.82050265 0.83518519 0.8332672\n",
      "        nan 0.84279762 0.86859127 0.86195767 0.85308862 0.82669974\n",
      " 0.84490741        nan 0.84367725 0.84847222 0.86800926 0.84156746\n",
      " 0.82563492 0.83554233        nan 0.82626984 0.84603175 0.86539683\n",
      " 0.8598545  0.82676587 0.85313492        nan 0.82119048 0.85328042\n",
      " 0.86977513 0.82016534 0.83095238 0.83388228        nan 0.8451918\n",
      " 0.86861111 0.86087963 0.84201058 0.8570172  0.84589286        nan\n",
      " 0.83846561 0.85545635 0.85833995 0.84656746 0.85121693 0.84655423\n",
      "        nan 0.83469577 0.85433201 0.83205688 0.79072751 0.82157407\n",
      " 0.80481481        nan 0.80164683 0.80974206 0.75531746 0.77265873\n",
      " 0.76553571 0.80150132        nan 0.81285714 0.78972222 0.83025132\n",
      " 0.80511243 0.77435185 0.8076455         nan 0.5315873  0.54938492\n",
      " 0.61929233 0.72546958 0.73779101 0.75279101        nan 0.83496693\n",
      " 0.85178571 0.82016534 0.80232143 0.82562169 0.83354497        nan\n",
      " 0.86312169 0.83452381 0.84814815 0.84239418 0.85572751 0.82876984\n",
      "        nan 0.87475529 0.88455026 0.83569444 0.84949735 0.83646164\n",
      " 0.87984127        nan 0.80050926 0.8248545  0.85505291 0.85490079\n",
      " 0.83500661 0.86763889        nan 0.81811508 0.82581349 0.84727513\n",
      " 0.83011905 0.83074074 0.84986772        nan 0.80866402 0.84568783\n",
      " 0.82492725 0.84861111 0.82533069 0.8235119         nan 0.84080688\n",
      " 0.85739418 0.85656746 0.87465608 0.87391534 0.86761905        nan\n",
      " 0.84289683 0.8452381  0.86717593 0.86955688 0.86706349 0.85181878\n",
      "        nan 0.82667989 0.84703042 0.82706349 0.80070767 0.81412698\n",
      " 0.79304233        nan 0.83705688 0.85213624 0.84762566 0.85761243\n",
      " 0.84390212 0.82972222        nan 0.84493386 0.84688492 0.86824735\n",
      " 0.86199735 0.85654101 0.84887566        nan 0.83579365 0.85892857\n",
      " 0.88998016 0.86238095 0.84087302 0.85746693        nan 0.84140212\n",
      " 0.84827381 0.81878307 0.84814815 0.8321627  0.82301587        nan\n",
      " 0.81268519 0.85262566 0.81164021 0.83420635 0.82741402 0.80683862\n",
      "        nan 0.84820106 0.86782407 0.85153439 0.84326058 0.81171296\n",
      " 0.84190476        nan 0.83529101 0.85712302 0.8595172  0.84169312\n",
      " 0.83642857 0.8478373         nan 0.8345172  0.81755952 0.81919312\n",
      " 0.82902778 0.80013228 0.8290873         nan 0.80131614 0.79447751\n",
      " 0.74765873 0.79777778 0.76255291 0.77274471        nan 0.79749339\n",
      " 0.79671958 0.81531746 0.7730291  0.81203042 0.79263889        nan\n",
      " 0.64242063 0.50816799 0.58578704 0.55056217 0.58259921 0.57699074\n",
      "        nan 0.82513228 0.81987434 0.79765212 0.76136905 0.82935185\n",
      " 0.79081349        nan 0.83926587 0.81326058 0.80986111 0.83204365\n",
      " 0.7755754  0.83423942        nan 0.83873016 0.8226918  0.79252646\n",
      " 0.82236772 0.8227381  0.80043651        nan 0.76277116 0.73037698\n",
      " 0.75487434 0.75656746 0.75943122 0.75816138        nan 0.8317328\n",
      " 0.8416336  0.85158069 0.82458995 0.83748016 0.80519841        nan\n",
      " 0.82326058 0.84646164 0.8320172  0.81287037 0.84841931 0.83683201\n",
      "        nan 0.87039683 0.86347222 0.85143519 0.86561508 0.85035714\n",
      " 0.81183862        nan 0.82878968 0.82615079 0.85955026 0.86463624\n",
      " 0.86896825 0.86505291        nan 0.82757275 0.83155423 0.82380952\n",
      " 0.79490741 0.8021627  0.80224206        nan 0.84142857 0.84490079\n",
      " 0.84980159 0.81205688 0.82970238 0.84707011        nan 0.81631614\n",
      " 0.84916667 0.85751323 0.85625    0.86364418 0.8721164         nan\n",
      " 0.82316799 0.84085979 0.85582672 0.8565873  0.8555291  0.86097884\n",
      "        nan 0.83962963 0.83021164 0.84525794 0.82914683 0.8014418\n",
      " 0.79685185        nan 0.82402116 0.84771164 0.86017196 0.83269841\n",
      " 0.82959656 0.81012566        nan 0.84392857 0.85541005 0.87984788\n",
      " 0.86945767 0.85221561 0.8574537         nan 0.83171958 0.84303571\n",
      " 0.86802249 0.84891534 0.83589286 0.85842593]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load data\n",
    "[X_train, X_test, y_train] = load_data()\n",
    "\n",
    "numeric_features = list(X_train.select_dtypes(include=['int64', 'float64']))\n",
    "X_train = min_max_scaling(X_train, numeric_features)\n",
    "X_test = min_max_scaling(X_test, numeric_features)\n",
    "\n",
    "# With Undersampling\n",
    "print(\"------- WITH UNDERSAMPLING -------\")\n",
    "pipeline = Pipeline([('under', RandomUnderSampler()), ('classification', SVC(probability=True))])\n",
    "\n",
    "param_grid = {'under__sampling_strategy': [0.1, 0.25, 0.5, 0.75, 0.9, 0.99, 'majority'],\n",
    "            'classification__C': [0.1, 1, 10, 100, 1000],\n",
    "            'classification__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "            'classification__kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "results_df = run_model(pipeline, param_grid, X_train, y_train, X_test)\n",
    "results_df.to_csv(\"./results/SVMUndersampling.csv\", index=None)\n",
    "\n",
    "print(\"-----------------------\\n\\n\")\n",
    "\n",
    "# Without Oversampling & Undersampling\n",
    "print(\"------- WITHOUT OVERSAMPLING & UNDERSAMPLING -------\")\n",
    "\n",
    "clf = Pipeline([('classification', SVC(probability=True))])\n",
    "\n",
    "param_grid = {'classification__C': [0.1, 1, 10, 100, 1000],\n",
    "              'classification__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'classification__kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "results_df = run_model(pipeline, param_grid, X_train, y_train, X_test)\n",
    "results_df.to_csv(\"./results/SVM.csv\", index=None)\n",
    "\n",
    "print(\"-----------------------\\n\\n\")\n",
    "\n",
    "# With Oversampling\n",
    "print(\"------- WITH OVERSAMPLING -------\")\n",
    "clf = Pipeline([('smote', SMOTE()), ('classification', SVC(probability=True))])\n",
    "\n",
    "weights = list(np.linspace(0.005, 0.25, 10)) + ['minority', 'auto']\n",
    "param_grid['smote__sampling_strategy'] = weights\n",
    "\n",
    "results_df = run_model(pipeline, param_grid, X_train, y_train, X_test)\n",
    "results_df.to_csv(\"./results/SVMWithSMOTE.csv\", index=None)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
